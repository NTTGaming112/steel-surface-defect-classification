# Steel Surface Defect Detection

Dá»± Ã¡n phÃ¡t hiá»‡n khuyáº¿t táº­t bá» máº·t thÃ©p sá»­ dá»¥ng Computer Vision vÃ  Machine Learning vá»›i nhiá»u mÃ´ hÃ¬nh khÃ¡c nhau.

## ğŸ“‹ MÃ´ táº£

Há»‡ thá»‘ng phÃ¡t hiá»‡n vÃ  phÃ¢n loáº¡i 6 loáº¡i khuyáº¿t táº­t trÃªn bá» máº·t thÃ©p:

- Crazing
- Inclusion
- Patches
- Pitted Surface
- Rolled-in Scale
- Scratches

## ğŸ¯ Dataset

**NEU Surface Defect Database**

- Training: 1440 áº£nh (240 áº£nh/class Ã— 6 classes)
- Validation: 360 áº£nh (60 áº£nh/class Ã— 6 classes)
- Tá»•ng: 1800 áº£nh
- KÃ­ch thÆ°á»›c: 200x200 pixels (grayscale sau preprocessing)

## ğŸš€ Models

### 1. SVM (Support Vector Machine)

- **Test Accuracy**: 98.33%
- **CV Accuracy**: 97.43%
- **Features**: SIFT + LBP (164 features)
- **Best Params**: kernel=linear, C=0.122
- **File**: `cv-project.ipynb`

### 2. Decision Tree

- **Test Accuracy**: 96.39%
- **CV Accuracy**: 95.49%
- **Features**: SIFT + LBP (164 features)
- **Best Params**: max_depth=28, criterion=entropy, min_samples_split=15
- **File**: `cv-project-decisiontree.ipynb`

### 3. KNN (K-Nearest Neighbors)

- **Test Accuracy**: 96.67%
- **CV Accuracy**: 95.97%
- **Features**: SIFT + LBP (164 features)
- **Best Params**: n_neighbors=6, metric=minkowski (p=1), algorithm=kd_tree
- **File**: `cv-project-knn.ipynb`

## ğŸ”§ Feature Extraction

### SIFT (Scale-Invariant Feature Transform)

- Bag of Visual Words vá»›i vocab_size=100
- MiniBatchKMeans clustering

### LBP (Local Binary Pattern)

- 8 points, radius 1
- 64-bin histogram

### Preprocessing

- CLAHE enhancement (clipLimit=2.0, tileGridSize=8x8)
- Resize to 200x200
- Grayscale conversion

## ğŸ“¦ CÃ i Ä‘áº·t

```bash
pip install opencv-python scikit-learn scikit-image gradio joblib pillow numpy pandas matplotlib seaborn tqdm optuna
```

## ğŸ® Sá»­ dá»¥ng

### 1. Training Models

Cháº¡y cÃ¡c notebook Ä‘á»ƒ train models:

```bash
jupyter notebook cv-project.ipynb          # SVM
jupyter notebook cv-project-decisiontree.ipynb  # Decision Tree
jupyter notebook cv-project-knn.ipynb      # KNN
```

### 2. Web Demo

Cháº¡y Gradio web interface:

```bash
python app.py
```

Truy cáº­p: http://127.0.0.1:7860

## ğŸ¨ Web Interface Features

- ğŸ“¤ Upload áº£nh defect
- ğŸ” Hiá»ƒn thá»‹ áº£nh sau tiá»n xá»­ lÃ½
- ğŸ¯ Top-3 predictions vá»›i confidence scores
- ğŸ“Š Detailed results table
- ğŸ¨ Custom gradient theme

## ğŸ“Š Performance Comparison

| Model         | CV Accuracy | Test Accuracy | N Trials | Best Params                     |
| ------------- | ----------- | ------------- | -------- | ------------------------------- |
| SVM           | 97.43%      | **98.33%**    | 100      | kernel=linear, C=0.122          |
| KNN           | 95.97%      | 96.67%        | 100      | k=6, metric=minkowski (p=1)     |
| Decision Tree | 95.49%      | 96.39%        | 100      | max_depth=28, criterion=entropy |

**Note**:

- Táº¥t cáº£ models sá»­ dá»¥ng combined features (SIFT BoVW 100 + LBP 64 = 164 features)
- Training set: 1440 samples, Test set: 360 samples
- Hyperparameter optimization: Optuna vá»›i TPE Sampler

## ğŸ”¬ Hyperparameter Optimization

Sá»­ dá»¥ng **Optuna** vá»›i:

- 100 trials cho má»—i feature set
- 3-fold cross-validation
- TPE Sampler
- Automatic checkpoint saving

## ğŸ“ Notes

- Models Ä‘Æ°á»£c train vá»›i scikit-learn 1.7.2
- Runtime cÃ³ thá»ƒ cÃ³ version warning (1.6.1)
- Táº¥t cáº£ models sá»­ dá»¥ng StandardScaler
- SIFT extractor Ä‘Æ°á»£c save Ä‘á»ƒ inference

## ğŸ“„ License

MIT License
