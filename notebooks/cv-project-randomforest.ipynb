{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75ed065d",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648fba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from skimage.feature import local_binary_pattern\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4cdcc4",
   "metadata": {},
   "source": [
    "## Configuration & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfe7a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH=\"data/NEU-DET/train/images\"\n",
    "TEST_PATH = \"data/NEU-DET/validation/images\"\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "MODEL_DIR = \"models\"\n",
    "\n",
    "LBP_PARAMS = {\n",
    "    \"param1\": {\n",
    "        'radius': 1,\n",
    "        'n_points': 8,\n",
    "        'method': 'default'\n",
    "    },\n",
    "    'param2': {\n",
    "        'radius': 1,\n",
    "        'n_points': 8,\n",
    "        'method': 'uniform'\n",
    "    }\n",
    "}\n",
    "\n",
    "SIFT_PARAMS = {\n",
    "    \"param1\": {\n",
    "        'vocab_size': 100\n",
    "    },\n",
    "    'param2': {\n",
    "        'vocab_size': 200\n",
    "    }\n",
    "}\n",
    "\n",
    "class_names = sorted([f for f in os.listdir(TRAIN_PATH) if os.path.isdir(os.path.join(TRAIN_PATH, f))])\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ca55ab",
   "metadata": {},
   "source": [
    "## Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813aa5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_path):\n",
    "    \"\"\"Load dataset and return DataFrame\"\"\"\n",
    "    data = []\n",
    "    class_names = sorted([f for f in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, f))])\n",
    "        \n",
    "    for label in class_names:\n",
    "        class_path = os.path.join(dataset_path, label)\n",
    "        image_files = [f for f in os.listdir(class_path) if f.endswith(('.jpg', '.png', '.bmp'))]\n",
    "        \n",
    "        for file_name in image_files:\n",
    "            file_path = os.path.join(class_path, file_name)\n",
    "            img = cv2.imread(file_path)\n",
    "            data.append((file_path, label, img.shape))\n",
    "            \n",
    "    return pd.DataFrame(data, columns=['filepath', 'label', 'shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cc6416",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = load_data(TRAIN_PATH)\n",
    "df_train.info()\n",
    "df_train['label'].value_counts().sort_index()\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59f2f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = load_data(TEST_PATH)\n",
    "df_test.info()\n",
    "df_test['label'].value_counts().sort_index()\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a276492",
   "metadata": {},
   "source": [
    "## Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576af7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "\n",
    "def preprocess_image(image):\n",
    "    \"\"\"Preprocess image: grayscale, resize, CLAHE\"\"\"\n",
    "    image = cv2.imread(image)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.resize(gray, (200, 200), interpolation=cv2.INTER_AREA)\n",
    "    gray = clahe.apply(gray)\n",
    "    return gray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32c1b88",
   "metadata": {},
   "source": [
    "## LBP Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60de33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lbp(image, radius=1, n_points=8, method='default'):\n",
    "    \"\"\"Extract LBP features with specified parameters\"\"\"\n",
    "    gray = preprocess_image(image)\n",
    "    lbp = local_binary_pattern(gray, n_points, radius, method=method)\n",
    "    \n",
    "    if method == 'uniform':\n",
    "        n_bins = n_points + 3\n",
    "    else:\n",
    "        n_bins = 2 ** n_points\n",
    "    \n",
    "    hist, _ = np.histogram(lbp.ravel(), bins=n_bins, range=(0, n_bins))\n",
    "    hist = hist.astype(\"float\")\n",
    "    hist /= (hist.sum() + 1e-6)\n",
    "    \n",
    "    return hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80731015",
   "metadata": {},
   "source": [
    "## SIFT BoW Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96152bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiftBowExtractor:\n",
    "    \"\"\"SIFT Bag-of-Words feature extractor\"\"\"\n",
    "    def __init__(self, vocab_size=100):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.kmeans = MiniBatchKMeans(n_clusters=self.vocab_size, \n",
    "                                      batch_size=200, \n",
    "                                      random_state=42,\n",
    "                                      n_init=10)\n",
    "        self.vocabulary = None\n",
    "\n",
    "    def _get_sift_descriptors(self, image):\n",
    "        sift = cv2.SIFT_create()\n",
    "        gray = preprocess_image(image)\n",
    "        _, descriptors = sift.detectAndCompute(gray, None)\n",
    "        return descriptors\n",
    "\n",
    "    def fit(self, image_paths):\n",
    "        \"\"\"Build vocabulary from training images\"\"\"\n",
    "        all_descriptors = []\n",
    "        \n",
    "        for img_path in tqdm(image_paths, desc=\"Building SIFT vocabulary\"):\n",
    "            descriptors = self._get_sift_descriptors(img_path)\n",
    "            if descriptors is not None:\n",
    "                all_descriptors.append(descriptors)\n",
    "            \n",
    "        all_descriptors = np.vstack(all_descriptors)\n",
    "        self.kmeans.fit(all_descriptors)\n",
    "        self.vocabulary = self.kmeans.cluster_centers_\n",
    "\n",
    "    def transform(self, image_paths):\n",
    "        \"\"\"Transform images to BoW histograms\"\"\"\n",
    "        final_features = []\n",
    "        \n",
    "        for img_path in tqdm(image_paths, desc=\"Extracting SIFT features\"):\n",
    "            descriptors = self._get_sift_descriptors(img_path)\n",
    "            hist = np.zeros(self.vocab_size, dtype=float)\n",
    "            \n",
    "            if descriptors is not None:\n",
    "                visual_words = self.kmeans.predict(descriptors)\n",
    "                hist, _ = np.histogram(visual_words, bins=np.arange(self.vocab_size + 1))\n",
    "                hist = hist.astype(float)\n",
    "                hist /= (hist.sum() + 1e-6)\n",
    "            \n",
    "            final_features.append(hist)\n",
    "            \n",
    "        return np.array(final_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f4d1f7",
   "metadata": {},
   "source": [
    "## Prepare Train/Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a071b805",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_paths = df_train['filepath'].tolist()\n",
    "y_train = df_train['label'].tolist()\n",
    "\n",
    "X_test_paths = df_test['filepath'].tolist()\n",
    "y_test = df_test['label'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f8f833",
   "metadata": {},
   "source": [
    "## Extract LBP Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc3d578",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbp_train = {}\n",
    "lbp_test = {}\n",
    "\n",
    "for param_name, params in LBP_PARAMS.items():\n",
    "    X_train_lbp = np.array([extract_lbp(p, **params) for p in tqdm(X_train_paths, desc=f\"LBP {param_name} train\")])\n",
    "    X_test_lbp = np.array([extract_lbp(p, **params) for p in tqdm(X_test_paths, desc=f\"LBP {param_name} test\")])\n",
    "    \n",
    "    lbp_train[param_name] = {'features': X_train_lbp, 'params': params}\n",
    "    lbp_test[param_name] = {'features': X_test_lbp, 'params': params}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca733603",
   "metadata": {},
   "source": [
    "## Extract SIFT Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8babefbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sift_train = {}\n",
    "sift_test = {}\n",
    "\n",
    "for param_name, params in SIFT_PARAMS.items():\n",
    "    extractor = SiftBowExtractor(**params)\n",
    "    \n",
    "    print(f\"\\nProcessing SIFT {param_name}...\")\n",
    "    extractor.fit(X_train_paths)\n",
    "    \n",
    "    X_train_sift = extractor.transform(X_train_paths)\n",
    "    X_test_sift = extractor.transform(X_test_paths)\n",
    "    \n",
    "    sift_train[param_name] = {'features': X_train_sift, 'params': params, 'extractor': extractor}\n",
    "    sift_test[param_name] = {'features': X_test_sift, 'params': params}\n",
    "    \n",
    "    extractor_path = f\"{MODEL_DIR}/sift_extractor_rf_{param_name}.pkl\"\n",
    "    joblib.dump(extractor, extractor_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5c0072",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a81abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(train_dict, test_dict):\n",
    "    \"\"\"Scale features for all parameter sets\"\"\"\n",
    "    scaled = {}\n",
    "    \n",
    "    for param_name in train_dict.keys():\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(train_dict[param_name]['features'])\n",
    "        X_test_scaled = scaler.transform(test_dict[param_name]['features'])\n",
    "        \n",
    "        scaled[param_name] = {\n",
    "            'train': X_train_scaled,\n",
    "            'test': X_test_scaled,\n",
    "            'scaler': scaler\n",
    "        }\n",
    "    \n",
    "    return scaled\n",
    "\n",
    "lbp_scaled = scale_features(lbp_train, lbp_test)\n",
    "sift_scaled = scale_features(sift_train, sift_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0868063",
   "metadata": {},
   "source": [
    "## Optuna Optimization Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5579a97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_rf(trial, X_train, y_train):\n",
    "    \"\"\"Optuna objective function for Random Forest optimization\"\"\"\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "    max_depth = trial.suggest_int('max_depth', 5, 30)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 5)\n",
    "    max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2'])\n",
    "    \n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "    return cv_scores.mean()\n",
    "\n",
    "def build_feature_sets():\n",
    "    \"\"\"Build all feature sets for optimization\"\"\"\n",
    "    feature_sets = {}\n",
    "    \n",
    "    for param_name in LBP_PARAMS.keys():\n",
    "        feat_name = f\"LBP_{param_name}\"\n",
    "        feature_sets[feat_name] = (\n",
    "            lbp_scaled[param_name]['train'], \n",
    "            lbp_scaled[param_name]['test'], \n",
    "            lbp_train[param_name]['params']\n",
    "        )\n",
    "    \n",
    "    for param_name in SIFT_PARAMS.keys():\n",
    "        feat_name = f\"SIFT_{param_name}\"\n",
    "        feature_sets[feat_name] = (\n",
    "            sift_scaled[param_name]['train'], \n",
    "            sift_scaled[param_name]['test'], \n",
    "            sift_train[param_name]['params']\n",
    "        )\n",
    "    \n",
    "    return feature_sets\n",
    "\n",
    "feature_sets = build_feature_sets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc98143",
   "metadata": {},
   "source": [
    "## Train & Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e27ba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_model(feat_name, X_train, X_test, y_train, y_test, feat_params):\n",
    "    \"\"\"Train model with Optuna and save artifacts\"\"\"\n",
    "    print(f\"\\nðŸ”„ {feat_name} | Shape: {X_train.shape}\")\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        study_name=f'RF_{feat_name}',\n",
    "        sampler=optuna.samplers.TPESampler(seed=42)\n",
    "    )\n",
    "    \n",
    "    study.optimize(\n",
    "        lambda trial: objective_rf(trial, X_train, y_train),\n",
    "        n_trials=100,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    best_model = RandomForestClassifier(**study.best_params, random_state=42, n_jobs=-1)\n",
    "    best_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = best_model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"âœ“ {feat_name}: CV={study.best_value:.4f} | Test={test_accuracy:.4f}\")\n",
    "    \n",
    "    joblib.dump(study, f\"{CHECKPOINT_DIR}/study_rf_{feat_name}.pkl\")\n",
    "    joblib.dump(best_model, f\"{MODEL_DIR}/best_rf_{feat_name}.pkl\")\n",
    "    \n",
    "    if feat_name.startswith(\"LBP\"):\n",
    "        param_name = feat_name.replace(\"LBP_\", \"\")\n",
    "        scaler = lbp_scaled[param_name]['scaler']\n",
    "    else:\n",
    "        param_name = feat_name.replace(\"SIFT_\", \"\")\n",
    "        scaler = sift_scaled[param_name]['scaler']\n",
    "    \n",
    "    joblib.dump(scaler, f\"{MODEL_DIR}/scaler_rf_{feat_name}.pkl\")\n",
    "    \n",
    "    metadata = {\n",
    "        'feature_set': feat_name,\n",
    "        'feature_params': feat_params,\n",
    "        'model_type': 'RandomForest',\n",
    "        'best_params': study.best_params,\n",
    "        'cv_accuracy': float(study.best_value),\n",
    "        'test_accuracy': float(test_accuracy),\n",
    "        'n_trials': len(study.trials),\n",
    "        'train_shape': X_train.shape,\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    with open(f\"{MODEL_DIR}/metadata_rf_{feat_name}.json\", 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    return {\n",
    "        'study': study,\n",
    "        'model': best_model,\n",
    "        'cv_accuracy': study.best_value,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'best_params': study.best_params,\n",
    "        'feature_params': feat_params\n",
    "    }\n",
    "\n",
    "optuna_results = {}\n",
    "for feat_name, (X_train, X_test, feat_params) in feature_sets.items():\n",
    "    optuna_results[feat_name] = train_and_save_model(\n",
    "        feat_name, X_train, X_test, y_train, y_test, feat_params\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8203a942",
   "metadata": {},
   "source": [
    "## Classification Reports - All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a5a6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat_name, result in optuna_results.items():\n",
    "    model = result['model']\n",
    "    \n",
    "    if feat_name.startswith(\"LBP\"):\n",
    "        param_name = feat_name.replace(\"LBP_\", \"\")\n",
    "        X_test_feat = lbp_scaled[param_name]['test']\n",
    "    else:\n",
    "        param_name = feat_name.replace(\"SIFT_\", \"\")\n",
    "        X_test_feat = sift_scaled[param_name]['test']\n",
    "    \n",
    "    y_pred = model.predict(X_test_feat)\n",
    "    \n",
    "    print(f\"Feature Set: {feat_name}\")\n",
    "    print(f\"Feature Params: {result['feature_params']}\")\n",
    "    print(f\"Best Model Params: {result['best_params']}\")\n",
    "    print(f\"CV Accuracy: {result['cv_accuracy']:.4f} | Test Accuracy: {result['test_accuracy']:.4f}\")\n",
    "    print(classification_report(y_test, y_pred, target_names=class_names))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
